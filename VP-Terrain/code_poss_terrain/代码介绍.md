# TerraFusion 代码分析文档

## 概述

本文档详细分析 TerraFusion 项目的四份核心代码文件，包括其工作流程、内容、作用、设计思路、与论文的对应关系、输入输出文件等。

---

## 目录

1. [整体流程图](#整体流程图)
2. [train_pro_contrastive.py](#1-train_pro_contrastivepy)
3. [train_only_vision.py](#2-train_only_visionpy)
4. [test_cluster_prediction.py](#3-test_cluster_predictionpy)
5. [test_only_vision.py](#4-test_only_visionpy)
6. [执行顺序与依赖关系](#执行顺序与依赖关系)
7. [文件依赖总览](#文件依赖总览)

---

## 整体流程图

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                          TerraFusion 完整流程                                │
└─────────────────────────────────────────────────────────────────────────────┘

                    ┌──────────────────────────────────┐
                    │     train_pro_contrastive.py     │
                    │     (自监督对比学习训练机体网络)    │
                    └──────────────────┬───────────────┘
                                       │
                    输出: only_pro.pth, new_lab.txt
                                       │
                                       ▼
┌──────────────────────────────────┐       ┌──────────────────────────────────┐
│     train_only_vision.py         │       │   test_cluster_prediction.py     │
│     (半监督训练视觉网络)           │       │   (测试机体网络，计算置信度)       │
└──────────────────┬───────────────┘       └──────────────────┬───────────────┘
                   │                                          │
        输出: only_vision.pth                    输出: pro_dark_conf, pro_dark_prediction
                   │                                          │
                   └─────────────────┬────────────────────────┘
                                     │
                                     ▼
                    ┌──────────────────────────────────┐
                    │       test_only_vision.py        │
                    │    (测试视觉网络 + 贝叶斯融合)     │
                    └──────────────────┬───────────────┘
                                       │
                    输出: vision_dark_conf, Fusion Acc
                                       │
                                       ▼
                         ┌─────────────────────┐
                         │     最终融合结果      │
                         │   TerraFusion Acc    │
                         └─────────────────────┘
```

---

## 1. train_pro_contrastive.py

### 1.1 基本信息

| 项目 | 内容 |
|------|------|
| **文件名** | `train_pro_contrastive.py` |
| **功能** | 使用自监督对比学习训练机体感知网络 (ProNet) |
| **对应论文章节** | Section III-B-1, Section III-C-2 |

### 1.2 工作流程

```
┌─────────────────────────────────────────────────────────────┐
│                 train_pro_contrastive.py                    │
└─────────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        ▼                     ▼                     ▼
┌───────────────┐    ┌───────────────┐    ┌───────────────┐
│ Step 1        │    │ Step 2        │    │ Step 3        │
│ 加载预训练     │ -> │ KMeans聚类    │ -> │ 对比学习训练   │
│ MobileNetV2   │    │ 生成伪标签     │    │ 机体网络       │
└───────────────┘    └───────────────┘    └───────────────┘
        │                     │                     │
   提取视觉特征          new_lab.txt           only_pro.pth
```

### 1.3 核心代码解析

#### Step 1: 使用预训练视觉模型提取特征

```python
model_vision = mobilenet_v2(pretrained=True).to(DEVICE)
model_vision.eval()
with torch.no_grad():
    for data in tqdm(train_loader):
        x_train, i_train, y_train = data
        i_train = i_train.to(DEVICE)
        extracted_feature = model_vision(i_train)  # 提取1000维视觉特征
        train_features.append(extracted_feature)
```

**对应论文**: Section III-B-1 (Self-Supervised Cross-Modal Alignment)
> "The prior knowledge of the pretrained vision encoder fI(·) is exploited to supervise the training of a proprioceptive encoder fS(·)"

#### Step 2: KMeans 聚类生成伪标签

```python
kmeans = KMeans(n_clusters=7, random_state=0).fit(train_features)
train_labels = kmeans.labels_  # 伪标签 (0-6)
np.savetxt(config.save_folder + 'new_lab.txt', train_labels, delimiter=',', fmt='%d')
```

**对应论文**: Section III-C-2 (Cluster-Guided Contrastive Learning)
> "the visual feature space YI^D is clustered using k-means, and the resulting pseudo-labels are assigned to the corresponding proprioceptive data"

#### Step 3: 对比学习训练机体网络

```python
loss_s = SupConLoss()  # 监督对比损失

for epoch in range(config.pro_epochs):
    for data in train_bar:
        x_train, i_train, y_train = data  # y_train 是伪标签
        y_train_pred, flat = model(x_train)  # CNN提取机体特征
        y_train_pred_lo = y_train_pred.unsqueeze(1)
        loss = loss_s(y_train_pred_lo, y_train)  # 对比损失
        loss.backward()
        optimizer.step()
```

**对应论文**: Section III-C-2, Equation (2)

$$\mathcal{L}_{cluster} = \sum_{i \in S} -\frac{1}{|P(i)|} \sum_{p \in P(i)} \log \frac{\exp(z_i \cdot z_p / \tau)}{\sum_{a \in P(i) \cup N(i)} \exp(z_i \cdot z_a / \tau)}$$

### 1.4 输入输出

#### 输入文件

| 文件 | 来源 | 说明 |
|------|------|------|
| `config.train_filename` | 数据集 | 训练集 HDF5 文件 (包含 images, signals, labels) |
| `config.valid_filename` | 数据集 | 验证集 HDF5 文件 |

#### 输出文件

| 文件 | 路径 | 说明 |
|------|------|------|
| `only_pro.pth` | `config.pro_save_path` | 训练好的机体网络权重 |
| `new_lab.txt` | `config.save_folder` | KMeans 生成的伪标签 |

### 1.5 核心思想

**自监督学习**：不使用人工标注，而是利用预训练视觉模型的知识来指导机体网络学习。

1. 视觉模型已经在 ImageNet 上学到了丰富的特征表示
2. 通过聚类将视觉特征分成7类，作为伪标签
3. 使用对比学习让机体网络学习到与视觉特征对齐的表征
4. 同类伪标签的样本在特征空间中被拉近，异类被推远

---

## 2. train_only_vision.py

### 2.1 基本信息

| 项目 | 内容 |
|------|------|
| **文件名** | `train_only_vision.py` |
| **功能** | 使用少量标注数据训练视觉网络 (VisionNet) |
| **对应论文章节** | Section III-B-2 (Semi-Supervised Semantic Alignment) |

### 2.2 工作流程

```
┌─────────────────────────────────────────────────────────────┐
│                   train_only_vision.py                      │
└─────────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        ▼                     ▼                     ▼
┌───────────────┐    ┌───────────────┐    ┌───────────────┐
│ Step 1        │    │ Step 2        │    │ Step 3        │
│ 随机采样α比例  │ -> │ 监督学习训练   │ -> │ 保存最优模型   │
│ 的标注数据     │    │ VisionNet     │    │               │
└───────────────┘    └───────────────┘    └───────────────┘
        │                     │                     │
   5%真实标签          CrossEntropyLoss       only_vision.pth
```

### 2.3 核心代码解析

#### Step 1: 随机采样部分标注数据

```python
def read_portion_data(filename, alpha):
    # 随机选取alpha比例的数据
    random_indices = random.sample(range(len(timeStamps)), int(len(timeStamps) * alpha))
    timeStamps = np.array([timeStamps[i] for i in random_indices])
    signals = np.array([signals[i] for i in random_indices])
    images = np.array([images[i] for i in random_indices])
    labels = np.array([labels[i] for i in random_indices])
    return [timeStamps, signals, images, labels]

# alpha = 0.05 表示只使用5%的标注数据
train_data = read_portion_data(config.train_filename, config.alpha)
```

**对应论文**: Section III-B-2
> "A small labeled subset Dl and unlabeled subset Du is used to train classification heads gI(·) and gS(·) for both modalities"

#### Step 2: 标准监督学习

```python
if config.vision_backbone == 'mobilenet':
    model = VisionNet()
elif config.vision_backbone == 'resnet':
    model = VisionNet_resnet()
elif config.vision_backbone == 'vit':
    model = VisionNet_vit()

loss_s = nn.CrossEntropyLoss()  # 交叉熵损失

for epoch in range(config.vision_epochs):
    for i, (_, imgs, labels) in enumerate(train_loader):
        outputs, vision_features = model(imgs)
        loss = loss_s(outputs, labels)  # 使用真实标签
        loss.backward()
        optimizer.step()
```

#### Step 3: 保存验证集最优模型

```python
if valid_acc > config.best_all_pre:
    config.best_all_pre = valid_acc
    torch.save(model.state_dict(), config.vision_save_path)
```

### 2.4 输入输出

#### 输入文件

| 文件 | 来源 | 说明 |
|------|------|------|
| `config.train_filename` | 数据集 | 训练集 HDF5 文件 |
| `config.valid_filename` | 数据集 | 验证集 HDF5 文件 |

#### 输出文件

| 文件 | 路径 | 说明 |
|------|------|------|
| `only_vision.pth` | `config.vision_save_path` | 训练好的视觉网络权重 |

### 2.5 核心思想

**半监督学习**：只使用少量 (5%) 的标注数据训练视觉分类器。

1. 使用预训练的 MobileNetV2/ResNet/ViT 作为骨干网络
2. 添加分类头进行7类地形分类
3. 使用交叉熵损失进行标准监督学习
4. 保存验证集准确率最高的模型

---

## 3. test_cluster_prediction.py

### 3.1 基本信息

| 项目 | 内容 |
|------|------|
| **文件名** | `test_cluster_prediction.py` |
| **功能** | 测试机体网络，计算类别中心和置信度 |
| **对应论文章节** | Section III-C-2, Section III-D-1 |

### 3.2 工作流程

```
┌─────────────────────────────────────────────────────────────┐
│                 test_cluster_prediction.py                  │
└─────────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        ▼                     ▼                     ▼
┌───────────────┐    ┌───────────────┐    ┌───────────────┐
│ Step 1        │    │ Step 2        │    │ Step 3        │
│ 训练集上计算   │ -> │ 聚类对齐得到   │ -> │ 测试集上计算   │
│ 特征向量       │    │ 类别中心       │    │ 余弦相似度     │
└───────────────┘    └───────────────┘    └───────────────┘
        │                     │                     │
   提取机体特征          7个类别中心          pro_dark_conf
```

### 3.3 核心代码解析

#### Step 1: 在训练集上提取特征

```python
model = CNN(config.feature_size, config.out_channels, config.output_size)
model.load_state_dict(torch.load(config.pro_save_path))  # 加载训练好的模型

model.eval()
with torch.no_grad():
    for data in tqdm(train_loader):
        x_train, i_train, y_train = data
        y_train_pred, out_flat = model(x_train)  # 提取特征
        train_feature.append(y_train_pred.detach().numpy())
```

#### Step 2: KMeans 聚类并对齐标签

```python
kmeans = KMeans(n_clusters=7, init='k-means++', random_state=0)
labels = kmeans.fit_predict(train_features)

# 把聚类标签对齐到真实标签
new_lab = np.zeros([len(train_features),])
for i in range(7):
    new_lab[labels == i] = np.argmax(np.bincount(train_labels[labels == i]))

# 计算每个类别的中心
centers = []
for label in set(new_lab):
    cluster_samples = train_features[new_lab == label]
    centers.append(np.mean(cluster_samples, axis=0))
```

**对应论文**: Section III-C-2
> "Starting from the labeled category centers Cl initialized as the averages of the labeled feature vectors in each category, the features of all the training proprioceptive signals are then clustered using the K-means method"

#### Step 3: 在测试集上计算余弦相似度作为置信度

```python
sim2center = np.zeros([len(result), 7])
for i, center in enumerate(centers):
    sim2center[:, i] = np.squeeze(cosine_similarity(result, [center]), axis=1)

# 保存置信度和预测结果
np.savetxt(config.save_folder + 'pro_dark_conf', sim2center, fmt='%.4f', delimiter=",")
np.savetxt(config.save_folder + 'pro_dark_prediction', max_sim2center, fmt='%d', delimiter=",")
```

**对应论文**: Section III-D-1, Equation (6)

$$\text{conf}_k^{pro}(c) = \frac{\exp(z_k \cdot \tilde{z}_c / T)}{\sum_{c'} \exp(z_k \cdot \tilde{z}_{c'} / T)}$$

### 3.4 输入输出

#### 输入文件

| 文件 | 来源 | 说明 |
|------|------|------|
| `config.train_filename` | 数据集 | 训练集 HDF5 文件 |
| `config.valid_filename` | 数据集 | 测试集 HDF5 文件 |
| `only_pro.pth` | `train_pro_contrastive.py` | 训练好的机体网络权重 |

#### 输出文件

| 文件 | 路径 | 说明 |
|------|------|------|
| `pro_dark_conf` | `config.save_folder` | 机体网络置信度矩阵 (N×7) |
| `pro_dark_conf_max` | `config.save_folder` | 每个样本的最大置信度 |
| `pro_dark_prediction` | `config.save_folder` | 机体网络预测类别 |

### 3.5 核心思想

**基于聚类中心的预测**：

1. 使用训练好的机体网络提取所有样本的特征向量
2. 对特征向量进行 KMeans 聚类，得到7个类别中心
3. 对于测试样本，计算其特征向量与每个类别中心的余弦相似度
4. 余弦相似度作为置信度，最大相似度对应的类别作为预测结果

---

## 4. test_only_vision.py

### 4.1 基本信息

| 项目 | 内容 |
|------|------|
| **文件名** | `test_only_vision.py` |
| **功能** | 测试视觉网络 + 贝叶斯决策融合 |
| **对应论文章节** | Section III-D (Confidence-Aware Multimodal Decision Fusion) |

### 4.2 工作流程

```
┌─────────────────────────────────────────────────────────────┐
│                    test_only_vision.py                      │
└─────────────────────────────────────────────────────────────┘
                              │
        ┌─────────────────────┼─────────────────────┐
        ▼                     ▼                     ▼
┌───────────────┐    ┌───────────────┐    ┌───────────────┐
│ Step 1        │    │ Step 2        │    │ Step 3        │
│ 加载VisionNet │ -> │ 测试集上计算   │ -> │ 贝叶斯融合     │
│ 计算视觉置信度 │    │ 视觉预测结果   │    │ 输出最终结果   │
└───────────────┘    └───────────────┘    └───────────────┘
        │                     │                     │
  only_vision.pth      vision_dark_conf        Fusion Acc
```

### 4.3 核心代码解析

#### Step 1: 测试视觉网络

```python
model = VisionNet()
model.load_state_dict(torch.load(config.vision_save_path))
model.to(DEVICE)

model.eval()
with torch.no_grad():
    for i, (_, imgs, labels) in enumerate(tqdm(test_loader)):
        outputs, vision_features = model(imgs)
        _, predicted = outputs.max(1)
        
        # 计算 Softmax 置信度 (温度 T=400)
        softmax_scores = F.softmax(outputs/400.0, dim=1)
        conf = np.concatenate((conf, softmax_scores.cpu().detach().numpy()))

# 保存视觉置信度和预测
np.savetxt(config.save_folder + 'vision_dark_conf', conf, fmt='%.4f', delimiter=",")
np.savetxt(config.save_folder + 'vision_dark_prediction', prediction, fmt='%d', delimiter=",")
```

**对应论文**: Section III-D-1, Equation (3)

$$\text{conf}_k^{vis}(c) = \frac{\exp(g_I(f_I(I_k), c) / T)}{\sum_{c'} \exp(g_I(f_I(I_k), c') / T)}$$

#### Step 2: 加载机体网络置信度

```python
pro_conf = np.loadtxt(config.save_folder + 'pro_dark_conf', delimiter=",")
vis_conf = np.loadtxt(config.save_folder + 'vision_dark_conf', delimiter=",")
```

#### Step 3: 贝叶斯决策融合

```python
# 先验概率 (均匀分布)
p_a = np.array([[1, 1, 1, 1, 1, 1, 1]])
p_a = p_a / 7

# 温度缩放 + Min-Max 归一化
pro_exp_cos_dis = np.exp(pro_conf / 0.3)
min_value = np.min(pro_exp_cos_dis)
max_value = np.max(pro_exp_cos_dis)
normalized_data = (pro_exp_cos_dis - min_value) / (max_value - min_value)

# 贝叶斯融合
result = normalized_data * vis_conf * p_a
a = np.argmax(result, axis=1)  # 融合预测结果

fusion_acc = np.sum(a == test_labels) / len(test_labels) * 100.
print('Fusion Acc: %.5f' % (fusion_acc))
```

**对应论文**: Section III-D-2, Equation (8) & (9)

$$P(c|I_k, S_k) = \eta P(c) P(I_k|c) P(S_k|c) = \eta P(c) \text{conf}_k^{vis}(c) \text{conf}_k^{pro}(c)$$

$$l_k = \arg\max_c P(c) \text{conf}_k^{vis}(c) \text{conf}_k^{pro}(c)$$

### 4.4 输入输出

#### 输入文件

| 文件 | 来源 | 说明 |
|------|------|------|
| `config.train_filename` | 数据集 | 训练集 HDF5 文件 |
| `config.valid_filename` | 数据集 | 测试集 HDF5 文件 |
| `only_vision.pth` | `train_only_vision.py` | 训练好的视觉网络权重 |
| `pro_dark_conf` | `test_cluster_prediction.py` | 机体网络置信度 |

#### 输出文件

| 文件 | 路径 | 说明 |
|------|------|------|
| `vision_dark_conf` | `config.save_folder` | 视觉网络置信度矩阵 (N×7) |
| `vision_dark_conf_max` | `config.save_folder` | 每个样本的最大置信度 |
| `vision_dark_prediction` | `config.save_folder` | 视觉网络预测类别 |
| **控制台输出** | - | Vision Acc, Fusion Acc |

### 4.5 核心思想

**贝叶斯决策融合**：

1. 分别获取视觉网络和机体网络的置信度
2. 使用贝叶斯原理，将先验概率与两个网络的置信度相乘
3. 选择后验概率最大的类别作为最终预测
4. 这种方式可以让可靠的模态主导决策，不可靠的模态影响减小

**融合的优势**：
- 正常光照下：视觉网络置信度高，主导预测
- 暗光条件下：视觉网络置信度低（几乎均匀），机体网络主导预测
- 自动适应不同环境条件

---

## 执行顺序与依赖关系

### 必须按照以下顺序执行

```bash
# Step 1: 训练机体网络 (自监督)
python train_pro_contrastive.py
# 输出: only_pro.pth, new_lab.txt

# Step 2: 训练视觉网络 (半监督)
python train_only_vision.py
# 输出: only_vision.pth

# Step 3: 测试机体网络
python test_cluster_prediction.py
# 输出: pro_dark_conf, pro_dark_prediction

# Step 4: 测试视觉网络 + 融合
python test_only_vision.py
# 输出: vision_dark_conf, vision_dark_prediction, Fusion Acc
```

### 依赖关系图

```
train_pro_contrastive.py
         │
         ├──→ only_pro.pth ──→ test_cluster_prediction.py
         │                              │
         └──→ new_lab.txt               ├──→ pro_dark_conf ─────┐
                                        │                       │
                                        └──→ pro_dark_prediction│
                                                                │
train_only_vision.py                                            │
         │                                                      │
         └──→ only_vision.pth ──→ test_only_vision.py ←─────────┘
                                        │
                                        ├──→ vision_dark_conf
                                        ├──→ vision_dark_prediction
                                        └──→ Fusion Acc (最终结果)
```

---

## 文件依赖总览

### 输入数据文件

| 文件类型 | Config 变量 | 示例路径 | 说明 |
|---------|-------------|----------|------|
| 训练集 | `config.train_filename` | `_train_7_new.hdf5` | 包含 images, signals, labels |
| 测试集 (Normal) | `config.valid_filename` | `_normal_7_new.hdf5` | 正常光照测试 |
| 测试集 (Dark) | `config.valid_filename` | `_dark_7_new.hdf5` | 暗光测试 |

### 中间产物文件

| 文件名 | 生成脚本 | 使用脚本 | 说明 |
|--------|----------|----------|------|
| `only_pro.pth` | `train_pro_contrastive.py` | `test_cluster_prediction.py` | 机体网络权重 |
| `new_lab.txt` | `train_pro_contrastive.py` | (可选) | 伪标签 |
| `only_vision.pth` | `train_only_vision.py` | `test_only_vision.py` | 视觉网络权重 |
| `pro_dark_conf` | `test_cluster_prediction.py` | `test_only_vision.py` | 机体置信度 |
| `pro_dark_prediction` | `test_cluster_prediction.py` | (分析用) | 机体预测结果 |

### 最终输出文件

| 文件名 | 生成脚本 | 说明 |
|--------|----------|------|
| `vision_dark_conf` | `test_only_vision.py` | 视觉网络置信度矩阵 |
| `vision_dark_prediction` | `test_only_vision.py` | 视觉网络预测结果 |
| **Fusion Acc** | `test_only_vision.py` | 融合准确率 (控制台输出) |

---

## 论文章节对应关系汇总

| 代码文件 | 论文章节 | 核心公式 |
|----------|----------|----------|
| `train_pro_contrastive.py` | Section III-B-1, III-C-2 | Equation (2) |
| `train_only_vision.py` | Section III-B-2 | 标准交叉熵 |
| `test_cluster_prediction.py` | Section III-C-2, III-D-1 | Equation (6), (7) |
| `test_only_vision.py` | Section III-D-1, III-D-2 | Equation (3), (8), (9) |

---

## 关键参数说明

| 参数 | Config 变量 | 默认值 | 说明 |
|------|-------------|--------|------|
| 标注比例 | `config.alpha` | 0.05 | 使用5%的标注数据 |
| 温度参数 (对比学习) | τ | 0.3 | SupConLoss 温度 |
| 温度参数 (融合) | T | 0.3 | 置信度缩放温度 |
| 机体网络训练轮数 | `config.pro_epochs` | 400 | - |
| 视觉网络训练轮数 | `config.vision_epochs` | 100 | - |
| 类别数 | - | 7 | 7种地形类型 |

---

## 总结

TerraFusion 框架通过四个核心脚本实现了：

1. **自监督学习** (`train_pro_contrastive.py`)：利用预训练视觉模型的知识，通过聚类生成伪标签，使用对比学习训练机体网络

2. **半监督学习** (`train_only_vision.py`)：仅使用5%的标注数据训练视觉分类网络

3. **基于聚类的预测** (`test_cluster_prediction.py`)：通过计算特征与类别中心的余弦相似度进行预测

4. **贝叶斯融合** (`test_only_vision.py`)：结合两个网络的置信度，实现鲁棒的多模态融合决策

这种设计使得模型能够：
- 减少对大量标注数据的依赖
- 在不同光照条件下保持鲁棒性
- 自动适应可靠模态进行决策
